{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:57.476643Z",
     "iopub.status.busy": "2024-08-11T16:12:57.476485Z",
     "iopub.status.idle": "2024-08-11T16:12:58.536094Z",
     "shell.execute_reply": "2024-08-11T16:12:58.535713Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:12.447360Z",
     "start_time": "2024-08-22T12:37:11.336844Z"
    }
   },
   "source": [
    "import os\n",
    "import contextlib\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "import torch\n",
    "import time\n",
    "from script.NeuralNets.Networks import SequentialNN\n",
    "from script.settings import device, data_type\n",
    "import script.DHOV.MultiDHOV as multidhov\n",
    "import gurobipy as grp\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from script.NeuralNets.ICNNFactory import ICNNFactory\n",
    "import matplotlib.pyplot as plt\n",
    "from script.Profiling import Timings\n",
    "import numpy as np\n",
    "import csv\n",
    "from script.DHOV.Sampling.PerGroupLineSearchSampling import PerGroupLineSearchSamplingStrategy\n",
    "from script.DHOV.Sampling.PerGroupSamplingStrategy import PerGroupSamplingStrategy\n",
    "from script.DHOV.Sampling.PerGroupFeasibleSamplingStrategy import PerGroupFeasibleSamplingStrategy\n",
    "from script.DHOV.Sampling.ZonotopeSamplingStrategy import ZonotopeSamplingStrategy"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ufuk/Documents/Programming/dhov_simple/script/Experiments/updated\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "f0822bc3823089a5",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.538604Z",
     "iopub.status.busy": "2024-08-11T16:12:58.537743Z",
     "iopub.status.idle": "2024-08-11T16:12:58.795658Z",
     "shell.execute_reply": "2024-08-11T16:12:58.795363Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:12.712125Z",
     "start_time": "2024-08-22T12:37:12.448009Z"
    }
   },
   "source": [
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "\n",
    "from vnnlib.compat import read_vnnlib_simple\n",
    "from collections import OrderedDict"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2cac90c448c123e2",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.797185Z",
     "iopub.status.busy": "2024-08-11T16:12:58.797110Z",
     "iopub.status.idle": "2024-08-11T16:12:58.798655Z",
     "shell.execute_reply": "2024-08-11T16:12:58.798498Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:12.714414Z",
     "start_time": "2024-08-22T12:37:12.712626Z"
    }
   },
   "source": [
    "onnx_name = \"mnist_relu_9_200.onnx\"\n",
    "vnnlib_name = \"1000_mnist_eps_015\"\n",
    "\n",
    "onnx_path = 'nets/' + onnx_name\n",
    "vnnlib_dir_path = \"specs/\" + vnnlib_name\n",
    "cpu_core_count = os.cpu_count()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "76f90072a2e894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:12.716096Z",
     "start_time": "2024-08-22T12:37:12.715089Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "7ee8af4f85372d15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:12.717409Z",
     "start_time": "2024-08-22T12:37:12.716441Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "43857ff221b9ea71",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import the onnx model and convert it to a SequentialNN\n",
    "It has to bee a SequentialNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bcacbdec4990e4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The SequentialNN needs to have one additional output layer, as its last layer never has relu activation but the imported network has. The last layer then gets initialized with the identity matrix and gets skipped during the verification"
   ]
  },
  {
   "cell_type": "code",
   "id": "7ece6141625b3884",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.799575Z",
     "iopub.status.busy": "2024-08-11T16:12:58.799452Z",
     "iopub.status.idle": "2024-08-11T16:12:58.808406Z",
     "shell.execute_reply": "2024-08-11T16:12:58.808198Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:12.818862Z",
     "start_time": "2024-08-22T12:37:12.806979Z"
    }
   },
   "source": [
    "onnx_model = onnx.load(onnx_path)\n",
    "pytorch_model = ConvertModel(onnx_model)\n",
    "nn = SequentialNN([28 * 28 * 1, 200, 200, 200, 200, 200, 200, 200, 200, 10, 10])\n",
    "\n",
    "\n",
    "parameter_list_onnx = list(pytorch_model.parameters())\n",
    "parameter_list_sequential = list(nn.parameters())\n",
    "for i in range(0, len(parameter_list_onnx), 2):\n",
    "    parameter_list_sequential[i].data = parameter_list_onnx[i].data\n",
    "    parameter_list_sequential[i+1].data = parameter_list_onnx[i+1].data\n",
    "parameter_list_sequential[-2].data = torch.eye(10, dtype=data_type)\n",
    "parameter_list_sequential[-1].data = torch.zeros(10, dtype=data_type)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/onnx2pytorch/convert/layer.py:29: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "2e0b30bcf0bc5ed3",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.809282Z",
     "iopub.status.busy": "2024-08-11T16:12:58.809143Z",
     "iopub.status.idle": "2024-08-11T16:12:58.810657Z",
     "shell.execute_reply": "2024-08-11T16:12:58.810465Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:13.031890Z",
     "start_time": "2024-08-22T12:37:13.030061Z"
    }
   },
   "source": [
    "parameter_list = list(pytorch_model.parameters()) # don't use nn here as we have added an extra layer\n",
    "output_size = 10\n",
    "number_layer = (len(parameter_list) - 2) // 2"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "32ac993914cba497",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load the data to apply the verification process for"
   ]
  },
  {
   "cell_type": "code",
   "id": "813c48a4e74d2cfd",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.811426Z",
     "iopub.status.busy": "2024-08-11T16:12:58.811361Z",
     "iopub.status.idle": "2024-08-11T16:12:58.812845Z",
     "shell.execute_reply": "2024-08-11T16:12:58.812671Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:13.409007Z",
     "start_time": "2024-08-22T12:37:13.407206Z"
    }
   },
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + .05  # revert normalization for viewing\n",
    "    npimg = img.to(\"cpu\").numpy()\n",
    "    plt.imshow(npimg, cmap=\"gray\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "e93b71692992f90e",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.813548Z",
     "iopub.status.busy": "2024-08-11T16:12:58.813485Z",
     "iopub.status.idle": "2024-08-11T16:12:58.845268Z",
     "shell.execute_reply": "2024-08-11T16:12:58.844975Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:13.607790Z",
     "start_time": "2024-08-22T12:37:13.574417Z"
    }
   },
   "source": [
    "transform = Compose([ToTensor()])\n",
    "training_data = MNIST(root=\"../../../mnist\", train=True, download=True, transform=transform)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "7708d17e62f8ed3c",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.846627Z",
     "iopub.status.busy": "2024-08-11T16:12:58.846406Z",
     "iopub.status.idle": "2024-08-11T16:12:58.848740Z",
     "shell.execute_reply": "2024-08-11T16:12:58.848579Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:13.743494Z",
     "start_time": "2024-08-22T12:37:13.741427Z"
    }
   },
   "source": [
    "def accuracy_test(model, converted_model, data):\n",
    "    total_correct = 0\n",
    "    total_wrong = 0\n",
    "    for image, label in data:\n",
    "        test_image = torch.unsqueeze(image, 0).to(dtype=data_type).to(device)   \n",
    "        model_test = model(test_image)\n",
    "        converted_test = converted_model(test_image)\n",
    "        if not torch.isclose(model_test, converted_test).all():\n",
    "            print(\"is not close\")\n",
    "            break\n",
    "        if torch.argmax(model_test).item() == label:\n",
    "            total_correct += 1\n",
    "        else:\n",
    "            total_wrong += 1\n",
    "    \n",
    "    print(\"is close\")    \n",
    "    print(f\"accuracy {total_correct/ len(training_data)}\")\n",
    "    \n",
    "\n",
    "\n",
    "do_test = False\n",
    "if do_test:\n",
    "    accuracy_test(pytorch_model, nn, training_data)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "18307fa1201eb3d6",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.849473Z",
     "iopub.status.busy": "2024-08-11T16:12:58.849409Z",
     "iopub.status.idle": "2024-08-11T16:12:58.852211Z",
     "shell.execute_reply": "2024-08-11T16:12:58.852060Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:13.915157Z",
     "start_time": "2024-08-22T12:37:13.912156Z"
    }
   },
   "source": [
    "def add_output_constraints(model, nn_layer_out_bounds, label, output_vars, sovler_bound=1e-3):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: the optimization problem in gurobi encoding the NN\n",
    "    :param nn_layer_out_bounds: torch.Tensor, approximating the upper and lower bounding the output layer of the NN\n",
    "    :param label: index of the label or target neuron which is compared against\n",
    "    :param output_vars: the gurobi variables from the model of the NN describing the output neurons of the NN\n",
    "    :param sovler_bound: provides a bound for the gurobi solver. If this bound is achieved, the optimizer stops\n",
    "    \"\"\"\n",
    "    \n",
    "    out_lb = nn_layer_out_bounds[-1][0].detach().cpu().numpy()\n",
    "    out_ub = nn_layer_out_bounds[-1][1].detach().cpu().numpy()\n",
    "    \n",
    "    difference_lb = out_lb - out_ub[label]\n",
    "    difference_ub = out_ub - out_lb[label]\n",
    "    difference_lb = difference_lb.tolist()\n",
    "    difference_ub = difference_ub.tolist()\n",
    "    \n",
    "    difference_lb.pop(label)\n",
    "    difference_ub.pop(label)\n",
    "    \n",
    "    min_diff = min(difference_lb)\n",
    "    max_diff = max(difference_ub)\n",
    "    \n",
    "    difference = model.addVars(9, lb=difference_lb, ub=difference_ub, name=\"diff_var\")\n",
    "    model.addConstrs((difference[i] == output_vars.tolist()[i] - output_vars.tolist()[label] for i in range(0, label)), name=\"diff_const0\")\n",
    "    model.addConstrs((difference[i - 1] == output_vars.tolist()[i] - output_vars.tolist()[label] for i in range(label + 1, 10)), name=\"diff_const1\")\n",
    "\n",
    "    max_var = model.addVar(lb=min_diff, ub=max_diff, name=\"max_var\")\n",
    "    model.addConstr(max_var == grp.max_(difference))\n",
    "\n",
    "    if sovler_bound != None:\n",
    "        model.setParam(\"BestObjStop\", sovler_bound)\n",
    "\n",
    "    model.update()\n",
    "    model.setObjective(max_var, grp.GRB.MAXIMIZE)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "d30a13deca625b13",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.852909Z",
     "iopub.status.busy": "2024-08-11T16:12:58.852842Z",
     "iopub.status.idle": "2024-08-11T16:12:58.854317Z",
     "shell.execute_reply": "2024-08-11T16:12:58.854176Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:14.079459Z",
     "start_time": "2024-08-22T12:37:14.077939Z"
    }
   },
   "source": [
    "def get_output_vars_dhov(model, output_size, output_layer_index):\n",
    "    output_vars = []\n",
    "    for i in range(output_size):\n",
    "        output_vars.append(model.getVarByName(\"output_layer_[{}]_[{}]\".format(output_layer_index, i)))\n",
    "    output_vars = grp.MVar.fromlist(output_vars)\n",
    "    return output_vars"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "dc139c863ddc89fa",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.855094Z",
     "iopub.status.busy": "2024-08-11T16:12:58.854983Z",
     "iopub.status.idle": "2024-08-11T16:12:58.859796Z",
     "shell.execute_reply": "2024-08-11T16:12:58.859550Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:14.261520Z",
     "start_time": "2024-08-22T12:37:14.256576Z"
    }
   },
   "source": [
    "def optimize_model(model, output_vars, start_overall_time, dhov_timings, time_limit=60*60, verbose=True, csv_to_write_to=None, csv_row_name=\"No name\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: the optimization problem in gurobi encoding the NN and the objective \n",
    "    :param output_vars: the gurobi variables from the model of the NN describing the output neurons of the NN\n",
    "    :return True if verification was successful, else false \n",
    "    \"\"\"\n",
    "    \n",
    "    model.setParam(\"TimeLimit\", time_limit)\n",
    "    \n",
    "    start_solving_time = time.time()\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_just_solving = end_time - start_solving_time\n",
    "    time_overall = end_time - start_overall_time\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"time for verification {}\".format(time_just_solving))\n",
    "        print(\"overall time {}\".format(time_overall))\n",
    "    \n",
    "    if model.Status == grp.GRB.OPTIMAL or model.Status == grp.GRB.USER_OBJ_LIMIT:\n",
    "                    \n",
    "        max_var = model.getVarByName(\"max_var\").getAttr(\"x\")\n",
    "        verification_successful = max_var < 0\n",
    "        \n",
    "        if csv_to_write_to is not None:\n",
    "            new_row = [csv_row_name, \"finished\", time_just_solving, time_overall, verification_successful, max_var] +  dhov_timings.get_all_results(do_round=True)\n",
    "            \n",
    "            with open(csv_to_write_to, 'a', newline='') as file_obj:\n",
    "                writer_object = csv.writer(file_obj)\n",
    "             \n",
    "                writer_object.writerow(new_row)\n",
    "                file_obj.close()\n",
    "        \n",
    "        if verbose:\n",
    "            for i, var in enumerate(output_vars.tolist()):\n",
    "                print(\"var {}: {}\".format(i, var.getAttr(\"x\")))\n",
    "                \n",
    "            if verification_successful:\n",
    "                print(\"property verified with max difference {}\".format(max_var))\n",
    "                return True\n",
    "            else:\n",
    "                 print(\"property NOT verified with max difference {}\".format(max_var))\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        return verification_successful\n",
    "    \n",
    "    elif model.Status == grp.GRB.TIME_LIMIT:        \n",
    "        \n",
    "        max_var_upper_bound = model.getAttr(\"ObjBound\")\n",
    "        \n",
    "        verification_failed_with_upper_bound = max_var_upper_bound > 0\n",
    "        \n",
    "        if verbose:\n",
    "            if verification_failed_with_upper_bound:\n",
    "                print(\"property NOT verified with upper bound for max difference {}\".format(max_var_upper_bound))\n",
    "            else:\n",
    "                print(\"time out and upper bound could not disprove the setting\")\n",
    "            \n",
    "        if csv_to_write_to is not None:\n",
    "            new_row = [csv_row_name, \"time_out\", time_just_solving, time_overall, verification_failed_with_upper_bound, max_var_upper_bound] + dhov_timings.get_all_results()\n",
    "            with open(csv_to_write_to, 'a', newline='') as file_obj:\n",
    "                writer_object = csv.writer(file_obj)\n",
    "             \n",
    "                writer_object.writerow(new_row)\n",
    "                file_obj.close()\n",
    "                \n",
    "                \n",
    "        return verification_failed_with_upper_bound\n",
    "\n",
    "    elif model.Status == grp.GRB.INFEASIBLE:\n",
    "        \n",
    "        if csv_to_write_to is not None:\n",
    "            new_row = [csv_row_name, \"infeasible\", time_just_solving, time_overall, False,]\n",
    "            with open(csv_to_write_to, 'a', newline='') as file_obj:\n",
    "                writer_object = csv.writer(file_obj)\n",
    "             \n",
    "                writer_object.writerow(new_row)\n",
    "                file_obj.close()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"model infeasible\")\n",
    "    \n",
    "            model.computeIIS()\n",
    "            print(\"constraint\")\n",
    "            all_constr = model.getConstrs()\n",
    "    \n",
    "            for const in all_constr:\n",
    "                if const.IISConstr:\n",
    "                    print(\"{}\".format(const))\n",
    "    \n",
    "            print(\"lower bound\")\n",
    "            all_var = model.getVars()\n",
    "            for var in all_var:\n",
    "                if var.IISLB:\n",
    "                    print(\"{}, lb: {}, ub: {}\".format(var, var.getAttr(\"lb\"), var.getAttr(\"ub\")))\n",
    "    \n",
    "            print(\"upper bound\")\n",
    "            all_var = model.getVars()\n",
    "            for var in all_var:\n",
    "                if var.IISUB:\n",
    "                    print(\"{}, lb: {}, ub: {}\".format(var, var.getAttr(\"lb\"), var.getAttr(\"ub\")))\n",
    "                \n",
    "        \n",
    "\n",
    "    return False"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "ee77c603c2d29b5c",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.860648Z",
     "iopub.status.busy": "2024-08-11T16:12:58.860506Z",
     "iopub.status.idle": "2024-08-11T16:12:58.862274Z",
     "shell.execute_reply": "2024-08-11T16:12:58.862114Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:14.444179Z",
     "start_time": "2024-08-22T12:37:14.442159Z"
    }
   },
   "source": [
    "def load_vnnlib_bounds(vnnlib_path, input_shape, n_out):\n",
    "    n_in = np.prod(input_shape)\n",
    "    res = read_vnnlib_simple(vnnlib_path, n_in, n_out)\n",
    "    bnds, spec = res[0]\n",
    "    \n",
    "    bnds = np.array(bnds)\n",
    "    lbs = bnds[:,0]\n",
    "    ubs = bnds[:,1]\n",
    "    \n",
    "    data_min = torch.tensor(lbs, dtype=data_type).reshape(input_shape).to(device)\n",
    "    data_max = torch.tensor(ubs, dtype=data_type).reshape(input_shape).to(device)\n",
    "\n",
    "    return [data_min, data_max]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "3b09947b5d45122e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# get bounds with crown"
   ]
  },
  {
   "cell_type": "code",
   "id": "554027a243f269d8",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.863133Z",
     "iopub.status.busy": "2024-08-11T16:12:58.862969Z",
     "iopub.status.idle": "2024-08-11T16:12:58.864428Z",
     "shell.execute_reply": "2024-08-11T16:12:58.864264Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:14.979184Z",
     "start_time": "2024-08-22T12:37:14.977471Z"
    }
   },
   "source": [
    "def onnx_to_bounded_model(onnx_path, input_shape):\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    torch_model = ConvertModel(onnx_model)\n",
    "    \n",
    "    x_concrete = torch.zeros(input_shape)\n",
    "    model = BoundedModule(torch_model, x_concrete)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "b42ade07de97121a",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.865135Z",
     "iopub.status.busy": "2024-08-11T16:12:58.865019Z",
     "iopub.status.idle": "2024-08-11T16:12:58.866849Z",
     "shell.execute_reply": "2024-08-11T16:12:58.866686Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:15.204919Z",
     "start_time": "2024-08-22T12:37:15.202781Z"
    }
   },
   "source": [
    "def load_vnnlib_spec_for_auto_lirpa(vnnlib_path, input_shape, n_out):\n",
    "    n_in = np.prod(input_shape)\n",
    "    res = read_vnnlib_simple(vnnlib_path, n_in, n_out)\n",
    "    bnds, spec = res[0]\n",
    "    \n",
    "    bnds = np.array(bnds)\n",
    "    lbs = bnds[:,0]\n",
    "    ubs = bnds[:,1]\n",
    "    \n",
    "    data_min = torch.tensor(lbs, dtype=data_type).reshape(input_shape)\n",
    "    data_max = torch.tensor(ubs, dtype=data_type).reshape(input_shape)\n",
    "    center = 0.5*(data_min + data_max)\n",
    "\n",
    "    ptb = PerturbationLpNorm(x_L=data_min, x_U=data_max)\n",
    "    x = BoundedTensor(center, ptb)\n",
    "    \n",
    "    return x, center"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "236afc883215fbaf",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.867560Z",
     "iopub.status.busy": "2024-08-11T16:12:58.867440Z",
     "iopub.status.idle": "2024-08-11T16:12:58.868755Z",
     "shell.execute_reply": "2024-08-11T16:12:58.868613Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:15.425748Z",
     "start_time": "2024-08-22T12:37:15.424103Z"
    }
   },
   "source": [
    "def get_layers(model):\n",
    "    return [l for l in model.nodes() if l.perturbed and \"input\" in l.name]"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "5e6033fd22f9b945",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.869389Z",
     "iopub.status.busy": "2024-08-11T16:12:58.869327Z",
     "iopub.status.idle": "2024-08-11T16:12:58.870750Z",
     "shell.execute_reply": "2024-08-11T16:12:58.870611Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:15.668429Z",
     "start_time": "2024-08-22T12:37:15.666585Z"
    }
   },
   "source": [
    "def get_intermediate_bounds(model):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing the concrete lower and upper bounds of each layer.\n",
    "    \n",
    "    Implemented own method to filter out bounds for weight matrices.\n",
    "    \n",
    "    Only call this method after compute_bounds()!\n",
    "    \"\"\"\n",
    "    od = OrderedDict()\n",
    "    for l in get_layers(model):\n",
    "        if hasattr(l, 'lower'):\n",
    "            od[l.name] = (l.lower, l.upper)\n",
    "            \n",
    "    return od"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "41468184eb04a1b1",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.871437Z",
     "iopub.status.busy": "2024-08-11T16:12:58.871321Z",
     "iopub.status.idle": "2024-08-11T16:12:58.873965Z",
     "shell.execute_reply": "2024-08-11T16:12:58.873780Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:15.872709Z",
     "start_time": "2024-08-22T12:37:15.869729Z"
    }
   },
   "source": [
    "def get_bounds_auto_lirpa(x: BoundedTensor, model: BoundedModule, method=\"crown\"):\n",
    "    model.compute_bounds(x=(x,), method=method, bound_lower=True, bound_upper=True)\n",
    "    bounds_dict_crown = get_intermediate_bounds(model)\n",
    "    crown_bounds_affine_out = []\n",
    "    prev_key = None\n",
    "    for i, key in enumerate(bounds_dict_crown.keys()):\n",
    "        if i == 0 and False: # use this if ibp is used (or i % 2 == 1:), and false since we use auto lirpa 0.5.0\n",
    "            continue\n",
    "        elif method == \"alpha-crown\" and \"59\" in key: # probably not relevant when using auto lirpa 0.5\n",
    "            # todo WTF, why do i need to do this?\n",
    "            lb, ub = bounds_dict_crown[key][0], bounds_dict_crown[prev_key][1]\n",
    "            crown_bounds_affine_out.append([lb.type(data_type).view(-1).to(device), ub.type(data_type).view(-1).to(device)])\n",
    "        else: \n",
    "            lb, ub = bounds_dict_crown[key]\n",
    "            crown_bounds_affine_out.append([lb.type(data_type).view(-1).to(device), ub.type(data_type).view(-1).to(device)])\n",
    "        prev_key = key\n",
    "        \n",
    "    crown_bounds_layer_out = []\n",
    "    relu = torch.nn.ReLU()\n",
    "    for i, (lb, ub) in enumerate(crown_bounds_affine_out):\n",
    "        if i == len(crown_bounds_affine_out) - 1:\n",
    "            crown_bounds_layer_out.append([lb, ub])\n",
    "        else:\n",
    "            lb_layer = relu(lb)\n",
    "            ub_layer = relu(ub)\n",
    "            crown_bounds_layer_out.append([lb_layer, ub_layer])\n",
    "            \n",
    "    return crown_bounds_affine_out, crown_bounds_layer_out"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "d53a434138b1b701",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create new csv"
   ]
  },
  {
   "cell_type": "code",
   "id": "2f97d8bc608761dc",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.874760Z",
     "iopub.status.busy": "2024-08-11T16:12:58.874628Z",
     "iopub.status.idle": "2024-08-11T16:12:58.876334Z",
     "shell.execute_reply": "2024-08-11T16:12:58.876188Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:17.039932Z",
     "start_time": "2024-08-22T12:37:17.037892Z"
    }
   },
   "source": [
    "def create_csv_file(csv_file_path, settings):\n",
    "    data = [ ['Input name', 'State of optimization', 'time for just solving', \"time overall\", \"was successful?\", \"max distance\"] +  Timings().get_ordering_as_list_of_strings() + [\"settings:\", settings] ]\n",
    "    # File path for the CSV file\n",
    "    if os.path.isfile(csv_file_path):\n",
    "        raise RuntimeError(\"File name {} already exists\".format(csv_file_path))\n",
    "    \n",
    "    # Open the file in write mode\n",
    "    with open(csv_file_path, mode='a+', newline='') as file_obj:\n",
    "        # Create a csv.writer object\n",
    "        writer_object = csv.writer(file_obj)\n",
    "        # Write data to the CSV file\n",
    "        writer_object.writerows(data)\n",
    "     \n",
    "    # Print a confirmation message\n",
    "    print(f\"CSV file '{csv_file_path}' created successfully.\")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "48adbb225ddba9f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### pre verification checks"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4e068c1d61a4714",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.877009Z",
     "iopub.status.busy": "2024-08-11T16:12:58.876941Z",
     "iopub.status.idle": "2024-08-11T16:12:58.878390Z",
     "shell.execute_reply": "2024-08-11T16:12:58.878235Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:17.535921Z",
     "start_time": "2024-08-22T12:37:17.534249Z"
    }
   },
   "source": [
    "def check_if_image_is_labeled_correctly(nn_model, image, label):\n",
    "    test_image = torch.unsqueeze(image, 0).to(dtype=data_type).to(device)\n",
    "    output = nn_model(test_image)\n",
    "    return torch.argmax(output).item() == label"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "4db18c087bbe41ee",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.879025Z",
     "iopub.status.busy": "2024-08-11T16:12:58.878965Z",
     "iopub.status.idle": "2024-08-11T16:12:58.880699Z",
     "shell.execute_reply": "2024-08-11T16:12:58.880552Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:17.826540Z",
     "start_time": "2024-08-22T12:37:17.824477Z"
    }
   },
   "source": [
    "def check_if_auto_lirpa_already_verifies(x, output_size, label, method=\"crown\"):\n",
    "    c_matrix = torch.zeros((1, output_size - 1, output_size))\n",
    "    for i in range(output_size - 1):\n",
    "        if i < label:\n",
    "            index = i\n",
    "        elif i >= label:\n",
    "            index = i + 1\n",
    "        \n",
    "        c_matrix[0][i][index] = 1\n",
    "        c_matrix[0][i][label] = -1\n",
    "    \n",
    "    lb, ub = model.compute_bounds(x=(x,), C=c_matrix, method=method)\n",
    "    \n",
    "    return ub.max() < 0"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "3acab87b527a78c2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Do Verification SNR with MILP in last layer"
   ]
  },
  {
   "cell_type": "code",
   "id": "5fa48a5e8cadf1f2",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.881427Z",
     "iopub.status.busy": "2024-08-11T16:12:58.881364Z",
     "iopub.status.idle": "2024-08-11T16:12:58.885997Z",
     "shell.execute_reply": "2024-08-11T16:12:58.885824Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:18.810660Z",
     "start_time": "2024-08-22T12:37:18.805727Z"
    }
   },
   "source": [
    "csv_file_path = \"snv+milp_mnist_9x200_eps_015.csv\"\n",
    "\n",
    "crown_method = \"alpha-crown\"\n",
    "tighten_bounds = True\n",
    "time_out = 60*60\n",
    "time_per_neuron_refinement = 10\n",
    "allow_heuristic_timeout_estimate = True\n",
    "settings = \"crown_method: {}, tighten_bounds: {}, time_out: {}, time_per_neuron_refinement: {}, allow_heuristic_timeout_estimate: {}\".format(crown_method, tighten_bounds, time_out, time_per_neuron_refinement, allow_heuristic_timeout_estimate)\n",
    "create_csv_file(csv_file_path, settings)\n",
    "\n",
    "\n",
    "for num, vnnlib_path in enumerate(sorted(os.listdir(vnnlib_dir_path))): \n",
    "    break\n",
    "    if num == 50:\n",
    "        break\n",
    "    \n",
    "    print(\"{} ================================================\".format(num))\n",
    "    \n",
    "    full_path = vnnlib_dir_path + \"/\" + vnnlib_path\n",
    "    input_bounds = load_vnnlib_bounds(full_path, [784,], 10)\n",
    "    model = onnx_to_bounded_model(onnx_path, [1,1,1,784])\n",
    "    image, label = training_data[num]\n",
    "    x, center = load_vnnlib_spec_for_auto_lirpa(full_path, [1,1,1,784], 10)\n",
    "    \n",
    "    if not check_if_image_is_labeled_correctly(nn, image, label):\n",
    "        print(\"skipped because wrong classification from the network\")\n",
    "        \n",
    "        new_row = [vnnlib_path, \"wrong classification\"]\n",
    "        with open(csv_file_path, 'a', newline='') as file_obj:\n",
    "            writer_object = csv.writer(file_obj)\n",
    "         \n",
    "            writer_object.writerow(new_row)\n",
    "            file_obj.close()\n",
    "            \n",
    "            \n",
    "        continue\n",
    "    \n",
    "    if check_if_auto_lirpa_already_verifies(x, 10, label, method=crown_method):\n",
    "        print(\"skipped because auto lirpa already verified\")\n",
    "        \n",
    "        new_row = [vnnlib_path, \"lirpa classified\"]\n",
    "        with open(csv_file_path, 'a', newline='') as file_obj:\n",
    "            writer_object = csv.writer(file_obj)\n",
    "         \n",
    "            writer_object.writerow(new_row)\n",
    "            file_obj.close()\n",
    "            \n",
    "        continue\n",
    "    \n",
    "    overall_time = time.time()\n",
    "    # weirdly the box bounds are faster for snr+milp\n",
    "    bounds_affine_out, bounds_layer_out = get_bounds_auto_lirpa(x, model, method=crown_method)\n",
    "    \n",
    "    # we need to pick these parameters, but for the SNR+MILP case these don't matter\n",
    "    sampling_strategy = PerGroupLineSearchSamplingStrategy(center, input_bounds, nn, sample_count=100)\n",
    "    group_size = 20\n",
    "    net_size = [5, 1]\n",
    "    icnn_factory = ICNNFactory(\"logical\", net_size, always_use_logical_layer=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dhov_verifier = multidhov.MultiDHOV()\n",
    "    \n",
    "    # block prints\n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "        dhov_verifier.start_verification(nn, center, icnn_factory, group_size, input_bounds, sampling_strategy, \n",
    "                                         init_affine_bounds=bounds_affine_out, init_layer_bounds=bounds_layer_out,\n",
    "                                         skip_last_layer=True,\n",
    "                                         tighten_bounds=tighten_bounds, \n",
    "                                         layers_as_snr=[0, 1, 2, 3, 4, 5, 6, 7], \n",
    "                                         layers_as_milp=[8],\n",
    "                                         time_out=time_out,\n",
    "                                         time_per_neuron_refinement=time_per_neuron_refinement,\n",
    "                                         allow_heuristic_timeout_estimate=allow_heuristic_timeout_estimate,)\n",
    "    \n",
    "\n",
    "    \n",
    "    dhov_model = dhov_verifier.nn_encoding_model.copy()\n",
    "    dhov_model.setParam(grp.GRB.Param.Threads, cpu_core_count)\n",
    "    dhov_model.update()\n",
    "    dhov_out_vars = get_output_vars_dhov(dhov_model, output_size, number_layer)\n",
    "    \n",
    "    add_output_constraints(dhov_model, dhov_verifier.bounds_layer_out, label, dhov_out_vars)\n",
    "    \n",
    "    optimize_model(dhov_model, dhov_out_vars, overall_time, dhov_verifier.timings, verbose=False, csv_to_write_to=csv_file_path, csv_row_name=vnnlib_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'snv+milp_mnist_9x200_eps_015.csv' created successfully.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "81f784dcbd39453c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Do DHOV Verification"
   ]
  },
  {
   "cell_type": "code",
   "id": "f62c64884d022034",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:12:58.886784Z",
     "iopub.status.busy": "2024-08-11T16:12:58.886716Z",
     "iopub.status.idle": "2024-08-11T16:15:53.385123Z",
     "shell.execute_reply": "2024-08-11T16:15:53.384776Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:37:23.025802Z",
     "start_time": "2024-08-22T12:37:20.032434Z"
    }
   },
   "source": [
    "csv_file_path = \"dhov_mnist_9x200_eps_015.csv\"\n",
    "\n",
    "\n",
    "group_size = 20\n",
    "sample_count = 3000\n",
    "crown_method = \"alpha-crown\"\n",
    "tighten_bounds = True\n",
    "time_out = 60*60\n",
    "time_per_neuron_refinement = 10\n",
    "time_per_icnn_refinement = 50\n",
    "allow_heuristic_timeout_estimate = True\n",
    "epochs = 200\n",
    "\n",
    "settings = \"group_size: {}, sample_count: {}, crown_method: {}, tighten_bounds: {}, time_out: {}, time_per_neuron_refinement: {}, time_per_icnn_refinement: {}, allow_heuristic_timeout_estimate: {}, epochs: {}\".format(\n",
    "    group_size, sample_count, crown_method, tighten_bounds, time_out, time_per_neuron_refinement, time_per_icnn_refinement, allow_heuristic_timeout_estimate, epochs)\n",
    "\n",
    "create_csv_file(csv_file_path, settings)\n",
    "for num, vnnlib_path in enumerate(sorted(os.listdir(vnnlib_dir_path))):\n",
    "    \n",
    "    if num == 50:\n",
    "        break\n",
    "        \n",
    "    print(\"{} ================================================\".format(num))\n",
    "    \n",
    "    full_path = vnnlib_dir_path + \"/\" + vnnlib_path\n",
    "    input_bounds = load_vnnlib_bounds(full_path, [784,], 10)\n",
    "    model = onnx_to_bounded_model(onnx_path, [1,1,1,784])\n",
    "    image, label = training_data[num]\n",
    "    x, center = load_vnnlib_spec_for_auto_lirpa(full_path, [1,1,1,784], 10)\n",
    "    \n",
    "    if not check_if_image_is_labeled_correctly(nn, image, label):\n",
    "        print(\"skipped because wrong classification from the network\")\n",
    "        \n",
    "        new_row = [vnnlib_path, \"wrong classification\"]\n",
    "        with open(csv_file_path, 'a', newline='') as file_obj:\n",
    "            writer_object = csv.writer(file_obj)\n",
    "         \n",
    "            writer_object.writerow(new_row)\n",
    "            file_obj.close()\n",
    "            \n",
    "            \n",
    "        continue\n",
    "    \n",
    "    if check_if_auto_lirpa_already_verifies(x, 10, label, method=crown_method):\n",
    "        print(\"skipped because auto lirpa already verified\")\n",
    "        \n",
    "        new_row = [vnnlib_path, \"lirpa classified\"]\n",
    "        with open(csv_file_path, 'a', newline='') as file_obj:\n",
    "            writer_object = csv.writer(file_obj)\n",
    "         \n",
    "            writer_object.writerow(new_row)\n",
    "            file_obj.close()\n",
    "            \n",
    "        continue\n",
    "    \n",
    "    overall_time = time.time()\n",
    "    \n",
    "    bounds_affine_out, bounds_layer_out = get_bounds_auto_lirpa(x, model, method=crown_method)\n",
    "    sampling_strategy = PerGroupLineSearchSamplingStrategy(center, input_bounds, nn, sample_count=sample_count)\n",
    "    net_size = [5, 1]\n",
    "    #icnn_factory = ICNNFactory(\"approx_max\", net_size, maximum_function=\"SMU\", function_parameter=0.3)\n",
    "    icnn_factory = ICNNFactory(\"logical\", net_size, always_use_logical_layer=False)\n",
    "    #icnn_factory = ICNNFactory(\"standard\", net_size, adapt_layer_for_init=True)\n",
    "    \n",
    "    \n",
    "    dhov_verifier = multidhov.MultiDHOV()\n",
    "    with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(f):\n",
    "            return_without_time_out = dhov_verifier.start_verification(nn, center, icnn_factory, group_size, input_bounds, sampling_strategy, \n",
    "                                                                    init_affine_bounds=bounds_affine_out, \n",
    "                                                                    init_layer_bounds=bounds_layer_out, \n",
    "                                                                    skip_last_layer=True,\n",
    "                                                                    icnn_epochs=epochs, \n",
    "                                                                    icnn_batch_size=10000, \n",
    "                                                                    use_over_approximation=True, \n",
    "                                                                    tighten_bounds=tighten_bounds, \n",
    "                                                                    use_fixed_neurons_in_grouping=False, \n",
    "                                                                    layers_as_snr=[], \n",
    "                                                                    layers_as_milp=[8], \n",
    "                                                                    force_inclusion_steps=3, \n",
    "                                                                    preemptive_stop=True, \n",
    "                                                                    grouping_method=\"consecutive\", \n",
    "                                                                    group_num_multiplier=None,\n",
    "                                                                    optimizer=\"SdLBFGS\", \n",
    "                                                                    init_network=True, \n",
    "                                                                    adapt_lambda=\"included\",\n",
    "                                                                    encode_icnn_enlargement_as_lp=False, \n",
    "                                                                    encode_relu_enlargement_as_lp=False,\n",
    "                                                                    time_out=time_out,\n",
    "                                                                    time_per_neuron_refinement=time_per_neuron_refinement,\n",
    "                                                                    time_per_icnn_refinement=time_per_icnn_refinement,\n",
    "                                                                    allow_heuristic_timeout_estimate=allow_heuristic_timeout_estimate,\n",
    "                                                                    break_after=None, print_training_loss=False, print_new_bounds=False, store_samples=False, print_optimization_steps=False, print_last_loss=False)\n",
    "    \n",
    "    if not return_without_time_out:\n",
    "        new_row = [vnnlib_path, \"time_out of DHOV\", \"-\", time.time() - overall_time, False, \"-\"] +  dhov_verifier.timings.get_all_results(do_round=True)\n",
    "        with open(csv_file_path, 'a', newline='') as file_obj:\n",
    "            writer_object = csv.writer(file_obj)\n",
    "         \n",
    "            writer_object.writerow(new_row)\n",
    "            file_obj.close()\n",
    "        continue\n",
    "        \n",
    "    dhov_model = dhov_verifier.nn_encoding_model.copy()\n",
    "    dhov_model.setParam(grp.GRB.Param.Threads, cpu_core_count)\n",
    "    dhov_model.update()\n",
    "    dhov_out_vars = get_output_vars_dhov(dhov_model, output_size, number_layer)\n",
    "    \n",
    "    add_output_constraints(dhov_model, dhov_verifier.bounds_layer_out, label, dhov_out_vars)\n",
    "    \n",
    "    optimize_model(dhov_model, dhov_out_vars, overall_time, dhov_verifier.timings, verbose=False, csv_to_write_to=csv_file_path, csv_row_name=vnnlib_path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'dhov_mnist_9x200_eps_015.csv' created successfully.\n",
      "0 ================================================\n",
      "skipped because wrong classification from the network\n",
      "1 ================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/vnnlib/compat.py:283: UserWarning: literal negation does not strictly follow SMT-LIB\n",
      "  ast_node = parse_file(vnnlib_filename, strict=False)\n",
      "/home/ufuk/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/onnx2pytorch/convert/model.py:167: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.experimental and inputs[0].shape[self.batch_dim] > 1:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/vnnlib/tokenizer.py:49\u001B[0m, in \u001B[0;36m__iter__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mStopIteration\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/vnnlib/tokenizer.py:49\u001B[0m, in \u001B[0;36m__iter__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mStopIteration\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/vnnlib/tokenizer.py:49\u001B[0m, in \u001B[0;36m__iter__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mStopIteration\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/vnnlib/tokenizer.py:49\u001B[0m, in \u001B[0;36m__iter__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mStopIteration\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 58\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m     56\u001B[0m overall_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 58\u001B[0m bounds_affine_out, bounds_layer_out \u001B[38;5;241m=\u001B[39m \u001B[43mget_bounds_auto_lirpa\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcrown_method\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m sampling_strategy \u001B[38;5;241m=\u001B[39m PerGroupLineSearchSamplingStrategy(center, input_bounds, nn, sample_count\u001B[38;5;241m=\u001B[39msample_count)\n\u001B[1;32m     60\u001B[0m net_size \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m1\u001B[39m]\n",
      "Cell \u001B[0;32mIn[17], line 2\u001B[0m, in \u001B[0;36mget_bounds_auto_lirpa\u001B[0;34m(x, model, method)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_bounds_auto_lirpa\u001B[39m(x: BoundedTensor, model: BoundedModule, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcrown\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_bounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbound_lower\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbound_upper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     bounds_dict_crown \u001B[38;5;241m=\u001B[39m get_intermediate_bounds(model)\n\u001B[1;32m      4\u001B[0m     crown_bounds_affine_out \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/auto_LiRPA/bound_general.py:1302\u001B[0m, in \u001B[0;36mBoundedModule.compute_bounds\u001B[0;34m(self, x, aux, C, method, IBP, forward, bound_lower, bound_upper, reuse_ibp, reuse_alpha, return_A, needed_A_dict, final_node_name, average_A, interm_bounds, reference_bounds, intermediate_constr, alpha_idx, aux_reference_bounds, need_A_only, cutter, decision_thresh, update_mask, ibp_nodes, cache_bounds)\u001B[0m\n\u001B[1;32m   1295\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(x\u001B[38;5;241m=\u001B[39mx, C\u001B[38;5;241m=\u001B[39mC, method\u001B[38;5;241m=\u001B[39mmethod, interm_bounds\u001B[38;5;241m=\u001B[39minterm_bounds,\n\u001B[1;32m   1296\u001B[0m     reference_bounds\u001B[38;5;241m=\u001B[39mreference_bounds, return_A\u001B[38;5;241m=\u001B[39mreturn_A,\n\u001B[1;32m   1297\u001B[0m     aux_reference_bounds\u001B[38;5;241m=\u001B[39maux_reference_bounds,\n\u001B[1;32m   1298\u001B[0m     needed_A_dict\u001B[38;5;241m=\u001B[39mneeded_A_dict,\n\u001B[1;32m   1299\u001B[0m     final_node_name\u001B[38;5;241m=\u001B[39mfinal_node_name,\n\u001B[1;32m   1300\u001B[0m     cutter\u001B[38;5;241m=\u001B[39mcutter, decision_thresh\u001B[38;5;241m=\u001B[39mdecision_thresh)\n\u001B[1;32m   1301\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bound_upper:\n\u001B[0;32m-> 1302\u001B[0m     ret2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_optimized_bounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbound_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mupper\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bound_lower:\n\u001B[1;32m   1304\u001B[0m     ret1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_optimized_bounds(bound_side\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlower\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/auto_LiRPA/optimized_bounds.py:714\u001B[0m, in \u001B[0;36m_get_optimized_bounds\u001B[0;34m(self, x, aux, C, IBP, forward, method, bound_side, reuse_ibp, return_A, average_A, final_node_name, interm_bounds, reference_bounds, aux_reference_bounds, needed_A_dict, cutter, decision_thresh, epsilon_over_decision_thresh)\u001B[0m\n\u001B[1;32m    709\u001B[0m         \u001B[38;5;28mbreakpoint\u001B[39m()\n\u001B[1;32m    711\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m iteration \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    712\u001B[0m     \u001B[38;5;66;03m# we do not need to update parameters in the last step since the\u001B[39;00m\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;66;03m# best result already obtained\u001B[39;00m\n\u001B[0;32m--> 714\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    716\u001B[0m     \u001B[38;5;66;03m# All intermediate variables are not needed at this point.\u001B[39;00m\n\u001B[1;32m    717\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clear_and_set_new(\n\u001B[1;32m    718\u001B[0m         \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    719\u001B[0m         cache_bounds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(apply_output_constraints_to) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m    720\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/test2_icnn/lib/python3.11/site-packages/torch/autograd/function.py:277\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBackwardCFunction\u001B[39;00m(_C\u001B[38;5;241m.\u001B[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;66;03m# _forward_cls is defined by derived class\u001B[39;00m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;66;03m# The user should define either backward or vjp but never both.\u001B[39;00m\n\u001B[1;32m    280\u001B[0m         backward_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mbackward  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    281\u001B[0m         vjp_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mvjp  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "a444d3433f05a154",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-08-11T16:15:53.386542Z",
     "iopub.status.busy": "2024-08-11T16:15:53.386445Z",
     "iopub.status.idle": "2024-08-11T16:15:53.388246Z",
     "shell.execute_reply": "2024-08-11T16:15:53.388079Z"
    },
    "ExecuteTime": {
     "end_time": "2024-08-22T12:33:37.714630Z",
     "start_time": "2024-08-22T12:33:37.713007Z"
    }
   },
   "source": [
    "print(\"done\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "920b00f2012e88a3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-22T12:33:38.382250Z",
     "start_time": "2024-08-22T12:33:38.380986Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b01ac8dd0de66cc0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
